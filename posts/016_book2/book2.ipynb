{
 "cells": [
  {
   "cell_type": "raw",
   "id": "223f6928-03cb-4f11-959f-8c57a18a865c",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Living With a Book: Blog 2\"\n",
    "description: \"Reinforcement learning with Actual Rewards vs. Expected Rewards and the impact on AI systems playing computer games\" \n",
    "author: \"Alina\"\n",
    "date: \"11/21/2025\"\n",
    "categories:\n",
    "  - LLMs\n",
    "  - Living With a Book\n",
    "  - Brief History of Intelligence\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5a89c6d-492d-4a81-84e4-59bf36f672a1",
   "metadata": {},
   "source": [
    "### Living With a Book Blog - Reinforced Learning and Game-playing AI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12cd838a-4b03-415b-a274-2788205edee1",
   "metadata": {},
   "source": [
    "![](chess.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0803db50-c199-4c0c-a742-b6125803cf23",
   "metadata": {},
   "source": [
    "Max Bennet details two types of reinforcemet learning, which is a type of machine learning where actions are performed to maximize a cumulative reward. A man by the name of Richard Sutton had proposed this idea (thought to be quite radical at the time) to reinforce behaviors based on *predicted* rewards rather than *actual* rewards. In other words, it would be rewarding the AI system when it thinks its winning instead of rewarding it when it actually does win. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24477f3b-4fda-424a-b318-45d52f4c493d",
   "metadata": {},
   "source": [
    "The book introduced two flow charts that greatly simplifies this strategy. When reinforcement learning is based on *actual* rewards, the learning doesn't actually come until the last move, the one that leads to the win or loss. Learning only occurs at the end based on actually winning or losing. On the other hand, for *expected* rewards, learning occurs at every step. There's a \"critic\" at each step, which basically predicts the likelihood of winning after that particular step. This then triggers a learning phase so that learning is continuous and based on a \"predicted likelihood of winning\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36941de6-0d90-4174-a418-7dacdf29e655",
   "metadata": {},
   "source": [
    "#### How does this apply to games? - Neurogammon --> TD-gammon"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4cbfac9-0b01-4edf-a8dc-f28c3abbe14b",
   "metadata": {},
   "source": [
    "For some background, Neurogammon was a backgammon-playing AI system created by Gerald Tesauro. The main issue with this AI system was that although it could beat every other backgammon-playing computer program, it could not even beat an intermediate human player. Tesaruo then put Sutton's idea to the test, employing \"temporal difference\" learning  to Neurogammon. It learned from trial and error instead of trying to play what it thought human expert players would play. It was actually super successful! It not only performed better than Neurogammon, but also it was as good as some of the best human players in the world. \n",
    "\n",
    "Sutton was the one who came up with the theory, but Tesauro proved that it was successful in practice. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f9e520c-763c-453d-bb7d-301e12419684",
   "metadata": {},
   "source": [
    "### My Thoughts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b99da441-dc3a-4407-9094-77bbfb55aaf2",
   "metadata": {},
   "source": [
    "I thought this was something really cool to think about, especially because chess.com has had such a big boom and we play against computers all the time with online games. To learn about the origins of some AI game-playing systems is really intriguing, especially since now I learned that it was modeled around a specific type of learning that mammals were theorized to have. I think there's a lot of potential for these systems to improve even more, especially with websites such as chess.com hosting an incredible amount of games per day with a good portion of them being humans against \"computers\". I'm curious to see if AI systems could ever be \"better\" than the best player in the world given that training is based on trial and error. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3-12 (COMM4190)",
   "language": "python",
   "name": "python3-12_comm4190"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
